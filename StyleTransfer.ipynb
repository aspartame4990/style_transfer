{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aspartame4990/style_transfer/blob/main/StyleTransfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm  # Use the Notebook-friendly progress bar\n",
        "from tensorboardX import SummaryWriter\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from io import BytesIO\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.models as models\n",
        "from torchvision.models import VGG16_Weights\n",
        "\n",
        "\n",
        "device=\"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(f\"âœ… Successfully connected to GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# 2. Define download paths\n",
        "data_dir = Path('./data/coco')\n",
        "data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 3. Download COCO 2017 Validation Set (as our training material)\n",
        "# This is a compromise: the full Train set is huge; the Val set has 5000 images and is enough for the Style Transfer demo\n",
        "coco_url = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
        "zip_path = data_dir / \"val2017.zip\"\n",
        "\n",
        "if not (data_dir / \"val2017\").exists():\n",
        "    print(\"â¬‡ï¸ Downloading the COCO dataset (~1GB), please wait...\")\n",
        "    !wget -q -P {data_dir} {coco_url}\n",
        "\n",
        "    print(\"ðŸ“¦ Extracting...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "\n",
        "    # Delete the zip to save space\n",
        "    os.remove(zip_path)\n",
        "    print(\"âœ… Dataset is ready!\")\n",
        "else:\n",
        "    print(\"âœ… Dataset already exists, skipping download.\")"
      ],
      "metadata": {
        "id": "low4ol2YQXRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Collect all image paths\n",
        "image_dir = data_dir / \"val2017\"\n",
        "all_image_paths = sorted(list(image_dir.glob(\"*.jpg\")))\n",
        "print(f\"Total number of images: {len(all_image_paths)}\")\n",
        "\n",
        "# 2. Split the dataset\n",
        "# First separate a test set (e.g., reserve 50 images for the final demo)\n",
        "train_val_paths, test_paths = train_test_split(all_image_paths, test_size=50, random_state=42)\n",
        "\n",
        "# Then split the rest into Train and Validation (e.g., 9:1)\n",
        "train_paths, val_paths = train_test_split(train_val_paths, test_size=0.1, random_state=42)\n",
        "\n",
        "print(f\"--- Dataset split details (record these numbers in the report) ---\")\n",
        "print(f\"ðŸ”¹ Training set (for training): {len(train_paths)} images\")\n",
        "print(f\"ðŸ”¸ Validation set (for plotting the loss curve): {len(val_paths)} images\")\n",
        "print(f\"ðŸš€ Test set (for the final showcase): {len(test_paths)} images\")\n",
        "\n",
        "\n",
        "# 3. Define the PyTorch Dataset class\n",
        "class ContentDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {img_path}: {e}\")\n",
        "            return torch.zeros(3, 1, 1)  # Return an all-black image to avoid crashes\n",
        "\n",
        "\n",
        "# 4. Define preprocessing (Transforms)\n",
        "# Style transfer usually resizes images to a fixed size (e.g., 256x256) and normalizes them\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0.0, std=0.05):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        # tensor is a [C, H, W] tensor and stays within 0 to 1\n",
        "        noise = torch.randn_like(tensor) * self.std + self.mean\n",
        "        noised = tensor + noise\n",
        "        # Keep it within the 0 to 1 range\n",
        "        return torch.clamp(noised, 0.0, 1.0)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}(mean={self.mean}, std={self.std})\"\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(256),\n",
        "    transforms.ToTensor(),\n",
        "    AddGaussianNoise(mean=0.0, std=0.05),\n",
        "])\n",
        "\n",
        "# 5. Create DataLoaders\n",
        "batch_size = 4  # Adjust based on Colab memory; if OOM, change to 2 or 1\n",
        "\n",
        "train_dataset = ContentDataset(train_paths, transform=train_transform)\n",
        "val_dataset = ContentDataset(val_paths, transform=train_transform)\n",
        "# Test dataset usually doesn't need a DataLoader; just read single images for display\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"âœ… DataLoaders are ready!\")"
      ],
      "metadata": {
        "id": "2M2P6douQcX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define the download function\n",
        "def load_image_from_url(url, target_size=None):\n",
        "    # Pretend to be a browser to avoid being blocked\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()  # Check for 403/404 errors\n",
        "\n",
        "        img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "\n",
        "        if target_size:\n",
        "            img = img.resize(target_size, Image.BICUBIC)\n",
        "        return img\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Download failed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# 2. Download 'Starry Night'\n",
        "# Backup URL (if Wikimedia still fails, use this fallback)\n",
        "style_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\"\n",
        "style_image_name = \"starry_night.jpg\"\n",
        "\n",
        "print(f\"â¬‡ï¸ Downloading style image: {style_image_name}...\")\n",
        "style_img_pil = load_image_from_url(style_url, target_size=(256, 256))\n",
        "\n",
        "if style_img_pil is not None:\n",
        "    style_img_pil.save(style_image_name)\n",
        "\n",
        "    # 3. Preprocess the style image\n",
        "    style_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Add the batch dimension\n",
        "    style_img_tensor = style_transform(style_img_pil).unsqueeze(0).to(device)\n",
        "    print(f\"âœ… Style image ready, tensor shape: {style_img_tensor.shape}\")\n",
        "\n",
        "    # 4. Visualize to verify\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(style_img_pil)\n",
        "    plt.title(\"Style Image: Starry Night\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"âš ï¸ Image download failed. Try another URL or manually upload 'starry_night.jpg' to the Colab file panel.\")"
      ],
      "metadata": {
        "id": "6-rwOlEnemKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. Basic components\n",
        "# ==========================================\n",
        "\n",
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.reflection_pad(x)\n",
        "        out = self.conv2d(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class LightweightConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        super(LightweightConvLayer, self).__init__()\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
        "\n",
        "        # 1. Depthwise convolution: groups=in_channels, extracts spatial features\n",
        "        # Followed by Norm, so bias=False\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size,\n",
        "                                   stride=stride, groups=in_channels, bias=False)\n",
        "\n",
        "        # [New] Insert Norm and Activation between them to increase nonlinearity\n",
        "        self.dw_norm = nn.InstanceNorm2d(in_channels, affine=True)\n",
        "        self.dw_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # 2. Pointwise convolution: 1x1 conv to combine channel features\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                                   stride=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.reflection_pad(x)\n",
        "\n",
        "        # Depthwise stage\n",
        "        out = self.depthwise(out)\n",
        "        out = self.dw_norm(out)\n",
        "        out = self.dw_relu(out)\n",
        "\n",
        "        # Pointwise stage\n",
        "        out = self.pointwise(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 2. Residual block\n",
        "# ==========================================\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Standard Residual Block\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in1 = nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in2 = nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.in1(self.conv1(x)))\n",
        "        out = self.in2(self.conv2(out))\n",
        "        out = out + residual\n",
        "        return out\n",
        "\n",
        "\n",
        "class LightweightResidualBlock(nn.Module):\n",
        "    \"\"\"Lightweight Residual Block (Depthwise Separable)\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super(LightweightResidualBlock, self).__init__()\n",
        "        # Replace the standard ConvLayer with LightweightConvLayer\n",
        "        self.conv1 = LightweightConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in1 = nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.conv2 = LightweightConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in2 = nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.in1(self.conv1(x)))\n",
        "        out = self.in2(self.conv2(out))\n",
        "        out = out + residual\n",
        "        return out\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 3. Upsampling layer\n",
        "# ==========================================\n",
        "\n",
        "class UpsampleConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
        "        super(UpsampleConvLayer, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_in = x\n",
        "        if self.upsample:\n",
        "            x_in = torch.nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n",
        "        out = self.reflection_pad(x_in)\n",
        "        out = self.conv2d(out)\n",
        "        return out\n",
        "\n",
        "class LightweightUpsampleConvLayer(nn.Module):\n",
        "    \"\"\"Lightweight upsampling layer: Upsample + Lightweight Conv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
        "        super(LightweightUpsampleConvLayer, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        # Reuse LightweightConvLayer\n",
        "        self.conv = LightweightConvLayer(in_channels, out_channels, kernel_size, stride)\n",
        "    def forward(self, x):\n",
        "        x_in = x\n",
        "        if self.upsample:\n",
        "            x_in = torch.nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n",
        "        out = self.conv(x_in)\n",
        "        return out\n",
        "\n",
        "# ==========================================\n",
        "# 4. Model definition\n",
        "# ==========================================\n",
        "\n",
        "class BaselineTransformerNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaselineTransformerNet, self).__init__()\n",
        "        # Initial convolution layers\n",
        "        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n",
        "        self.in1 = nn.InstanceNorm2d(32, affine=True)\n",
        "        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
        "        self.in2 = nn.InstanceNorm2d(64, affine=True)\n",
        "        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n",
        "        self.in3 = nn.InstanceNorm2d(128, affine=True)\n",
        "\n",
        "        # Residual layers (Standard)\n",
        "        self.res1 = ResidualBlock(128)\n",
        "        self.res2 = ResidualBlock(128)\n",
        "        self.res3 = ResidualBlock(128)\n",
        "        self.res4 = ResidualBlock(128)\n",
        "        self.res5 = ResidualBlock(128)\n",
        "\n",
        "        # Upsampling Layers\n",
        "        self.deconv1 = UpsampleConvLayer(128, 64, kernel_size=3, stride=1, upsample=2)\n",
        "        self.in4 = nn.InstanceNorm2d(64, affine=True)\n",
        "        self.deconv2 = UpsampleConvLayer(64, 32, kernel_size=3, stride=1, upsample=2)\n",
        "        self.in5 = nn.InstanceNorm2d(32, affine=True)\n",
        "        self.deconv3 = ConvLayer(32, 3, kernel_size=9, stride=1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, X):\n",
        "        y = self.relu(self.in1(self.conv1(X)))\n",
        "        y = self.relu(self.in2(self.conv2(y)))\n",
        "        y = self.relu(self.in3(self.conv3(y)))\n",
        "        y = self.res1(y)\n",
        "        y = self.res2(y)\n",
        "        y = self.res3(y)\n",
        "        y = self.res4(y)\n",
        "        y = self.res5(y)\n",
        "        y = self.relu(self.in4(self.deconv1(y)))\n",
        "        y = self.relu(self.in5(self.deconv2(y)))\n",
        "        y = self.deconv3(y)\n",
        "        return torch.sigmoid(y)\n",
        "\n",
        "class LightweightTransformerNet(nn.Module):\n",
        "    def __init__(self, mode='v1'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            mode (str):\n",
        "                'v1' - Only Residual Blocks use lightweight conv (Baseline Encoder/Decoder)\n",
        "                'v2' - Encoder + Residual Blocks use lightweight conv\n",
        "                'v3' - Fully lightweight (Encoder + Res + Decoder)\n",
        "        \"\"\"\n",
        "        super(LightweightTransformerNet, self).__init__()\n",
        "        self.mode = mode\n",
        "        print(f\"ðŸ—ï¸ Initializing Lightweight Model with mode: {mode}\")\n",
        "\n",
        "        # 1. Dynamically choose layer types (Dynamic Layer Selection)\n",
        "        # ---------------------------------------------------\n",
        "        # Encoder Layer Type\n",
        "        if mode in ['v2', 'v3']:\n",
        "            EncLayer = LightweightConvLayer\n",
        "        else:\n",
        "            EncLayer = ConvLayer  # v1 uses standard conv as the encoder\n",
        "\n",
        "        # Decoder Layer Type\n",
        "        if mode == 'v3':\n",
        "            UpLayer = LightweightUpsampleConvLayer\n",
        "            FinalLayer = LightweightConvLayer\n",
        "        else:\n",
        "            UpLayer = UpsampleConvLayer # v1, v2 use standard conv as the decoder\n",
        "            FinalLayer = ConvLayer\n",
        "\n",
        "        # ---------------------------------------------------\n",
        "\n",
        "        # Initial convolution layers (Encoder)\n",
        "        self.conv1 = EncLayer(3, 32, kernel_size=9, stride=1)\n",
        "        self.in1 = nn.InstanceNorm2d(32, affine=True)\n",
        "        self.conv2 = EncLayer(32, 64, kernel_size=3, stride=2)\n",
        "        self.in2 = nn.InstanceNorm2d(64, affine=True)\n",
        "        self.conv3 = EncLayer(64, 128, kernel_size=3, stride=2)\n",
        "        self.in3 = nn.InstanceNorm2d(128, affine=True)\n",
        "\n",
        "        # Residual layers\n",
        "        self.res1 = LightweightResidualBlock(128)\n",
        "        self.res2 = LightweightResidualBlock(128)\n",
        "        self.res3 = LightweightResidualBlock(128)\n",
        "        self.res4 = LightweightResidualBlock(128)\n",
        "        self.res5 = LightweightResidualBlock(128)\n",
        "\n",
        "        # Upsampling Layers (Decoder)\n",
        "        self.deconv1 = UpLayer(128, 64, kernel_size=3, stride=1, upsample=2)\n",
        "        self.in4 = nn.InstanceNorm2d(64, affine=True)\n",
        "        self.deconv2 = UpLayer(64, 32, kernel_size=3, stride=1, upsample=2)\n",
        "        self.in5 = nn.InstanceNorm2d(32, affine=True)\n",
        "\n",
        "        # Final Output Layer\n",
        "        self.deconv3 = FinalLayer(32, 3, kernel_size=9, stride=1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, X):\n",
        "        y = self.relu(self.in1(self.conv1(X)))\n",
        "        y = self.relu(self.in2(self.conv2(y)))\n",
        "        y = self.relu(self.in3(self.conv3(y)))\n",
        "        y = self.res1(y)\n",
        "        y = self.res2(y)\n",
        "        y = self.res3(y)\n",
        "        y = self.res4(y)\n",
        "        y = self.res5(y)\n",
        "        y = self.relu(self.in4(self.deconv1(y)))\n",
        "        y = self.relu(self.in5(self.deconv2(y)))\n",
        "        y = self.deconv3(y)\n",
        "        return torch.sigmoid(y)\n",
        "\n",
        "# ==========================================\n",
        "# Visualization\n",
        "# ==========================================\n",
        "\n",
        "def register_feature_hooks(model, layer_names):\n",
        "    \"\"\"\n",
        "    model: any nn.Module\n",
        "    layer_names: list of layer names to inspect, e.g., [\"conv1\", \"res1\", \"deconv1\"]\n",
        "    Returns: feature_maps dict and hooks list\n",
        "    \"\"\"\n",
        "    feature_maps = {}\n",
        "\n",
        "    def get_hook(name):\n",
        "        def hook(module, input, output):\n",
        "            # Save a copy to the CPU for visualization\n",
        "            feature_maps[name] = output.detach().cpu()\n",
        "        return hook\n",
        "\n",
        "    hooks = []\n",
        "    for name, module in model.named_modules():\n",
        "        if name in layer_names:\n",
        "            hooks.append(module.register_forward_hook(get_hook(name)))\n",
        "    return feature_maps, hooks\n",
        "\n",
        "# ==========================================\n",
        "# Verify parameter counts\n",
        "# ==========================================\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "baseline = BaselineTransformerNet()\n",
        "v1 = LightweightTransformerNet(\"v1\")\n",
        "v2 = LightweightTransformerNet(\"v2\")\n",
        "v3 = LightweightTransformerNet(\"v3\")\n",
        "\n",
        "params_baseline = count_parameters(baseline)\n",
        "params_v1 = count_parameters(v1)\n",
        "params_v2 = count_parameters(v2)\n",
        "params_v3 = count_parameters(v3)\n",
        "\n",
        "print(f\"ðŸ“Š Baseline Model Parameters: {params_baseline:,}\")\n",
        "print(f\"ðŸš€ Lightweight-V1 Model Parameters: {params_v1:,}\")\n",
        "print(f\"ðŸ“‰ Reduction: {(1 - params_v1 / params_baseline) * 100:.2f}%\")\n",
        "print(f\"ðŸš€ Lightweight-V2 Model Parameters: {params_v2:,}\")\n",
        "print(f\"ðŸ“‰ Reduction: {(1 - params_v2 / params_baseline) * 100:.2f}%\")\n",
        "print(f\"ðŸš€ Lightweight-V3 Model Parameters: {params_v3:,}\")\n",
        "print(f\"ðŸ“‰ Reduction: {(1 - params_v3 / params_baseline) * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "iW-EvH5gepuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def denormalize_img(tensor):\n",
        "    \"\"\"\n",
        "    Input: Tensor (B, C, H, W) or (C, H, W) range [0, 1]\n",
        "    Output: Numpy Array (H, W, C) range [0, 255]\n",
        "    \"\"\"\n",
        "    img = tensor.clone().detach().cpu()\n",
        "\n",
        "    if img.dim() == 4:\n",
        "        img = img[0]\n",
        "\n",
        "    img = img.permute(1, 2, 0).numpy()\n",
        "\n",
        "    # Only need to clip for safety; mean/std computation is no longer required\n",
        "    img = np.clip(img, 0, 1)\n",
        "    img = (img * 255).astype(np.uint8)\n",
        "\n",
        "    return img"
      ],
      "metadata": {
        "id": "MXhRJgxFBqQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gram_matrix(input):\n",
        "    batch_size, c, h, w = input.size()\n",
        "    features = input.view(batch_size, c, h * w)\n",
        "    G = torch.bmm(features, features.transpose(1, 2))\n",
        "    return G.div(c * h * w)\n",
        "\n",
        "class Vgg16Features(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Vgg16Features, self).__init__()\n",
        "        vgg16 = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
        "        self.features = vgg16.features.eval()\n",
        "        for param in self.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Define the normalization parameters required by VGG\n",
        "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
        "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Manually normalize before feeding into VGG\n",
        "        # x is assumed to be in [0, 1]\n",
        "        x = (x - self.mean) / self.std\n",
        "\n",
        "        features = []\n",
        "        style_layers_indices = {3, 8, 15, 22}\n",
        "        for name, layer in self.features._modules.items():\n",
        "            x = layer(x)\n",
        "            if int(name) in style_layers_indices:\n",
        "                features.append(x)\n",
        "        return features\n",
        "\n",
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self, style_img_tensor, content_weight=1e5, style_weight=1e10, tv_weight=0):\n",
        "        super(PerceptualLoss, self).__init__()\n",
        "        self.vgg = Vgg16Features().to(device)\n",
        "        self.content_weight = content_weight\n",
        "        self.style_weight = style_weight\n",
        "        self.tv_weight = tv_weight\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "        # Compute style Grams (style_img_tensor is now in 0-1; VGG will normalize internally)\n",
        "        with torch.no_grad():\n",
        "            style_img_tensor = style_img_tensor.to(device)\n",
        "            self.style_features = self.vgg(style_img_tensor)\n",
        "            self.style_grams = [gram_matrix(f).detach() for f in self.style_features]\n",
        "\n",
        "    def total_variation_loss(self, img):\n",
        "        bs_img, c_img, h_img, w_img = img.size()\n",
        "        tv_h = torch.pow(img[:, :, 1:, :] - img[:, :, :-1, :], 2).sum()\n",
        "        tv_w = torch.pow(img[:, :, :, 1:] - img[:, :, :, :-1], 2).sum()\n",
        "        return (tv_h + tv_w) / (bs_img * c_img * h_img * w_img)\n",
        "\n",
        "    def forward(self, generated_img, content_img):\n",
        "        # generated_img and content_img are both in [0, 1]\n",
        "        current_batch_size = generated_img.shape[0]\n",
        "\n",
        "        gen_features = self.vgg(generated_img)\n",
        "        content_features = self.vgg(content_img)\n",
        "\n",
        "        # Content Loss (commonly relu2_2, i.e., index 1 in the list)\n",
        "        content_loss = self.content_weight * self.mse_loss(gen_features[1], content_features[1])\n",
        "\n",
        "        # Style Loss\n",
        "        style_loss = 0.0\n",
        "        for gen_feat, target_gram in zip(gen_features, self.style_grams):\n",
        "            gen_gram = gram_matrix(gen_feat)\n",
        "            target_gram_expanded = target_gram.expand(current_batch_size, -1, -1)\n",
        "            style_loss += self.mse_loss(gen_gram, target_gram_expanded)\n",
        "\n",
        "        style_loss *= self.style_weight\n",
        "\n",
        "        # Total Variation Loss\n",
        "        tv_loss = self.tv_weight * self.total_variation_loss(generated_img)\n",
        "\n",
        "        total_loss = content_loss + style_loss + tv_loss\n",
        "\n",
        "        return total_loss, content_loss, style_loss, tv_loss\n",
        "\n",
        "print(\"âœ… Loss function updated with normalization logic!\")"
      ],
      "metadata": {
        "id": "hF1mCfqapn9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_style_transfer(model, model_name, train_loader, style_img_tensor,\n",
        "                         fixed_content_image,\n",
        "                         scheduler_patience,\n",
        "                         scheduler_factor,\n",
        "                         val_loader,\n",
        "                         val_interval,\n",
        "                         val_batches,\n",
        "                         epochs=2,\n",
        "                         lr=1e-3,\n",
        "                         content_weight=1e5,\n",
        "                         style_weight=1e10,\n",
        "                         tv_weight=0,\n",
        "                         save_dir='./checkpoints'\n",
        "                         ):\n",
        "\n",
        "    import os\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Initialize ReduceLROnPlateau scheduler\n",
        "    # mode='min': triggers when the monitored metric (loss) stops decreasing\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                  factor=scheduler_factor,\n",
        "                                  patience=scheduler_patience\n",
        "                                  )\n",
        "\n",
        "    criterion = PerceptualLoss(style_img_tensor, content_weight, style_weight, tv_weight)\n",
        "\n",
        "    log_dir = os.path.join(\"runs\", model_name)\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "    print(f\"ðŸ“ TensorBoard log directory: {log_dir}\")\n",
        "\n",
        "    loss_history = []\n",
        "    start_time = time.time()\n",
        "    current_step = 0\n",
        "\n",
        "    print(f\"ðŸš€ Starting training: {model_name} | Epochs: {epochs}\")\n",
        "\n",
        "    # Log the original image first\n",
        "    writer.add_image('images/content', denormalize_img(fixed_content_image), dataformats='HWC')\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\")\n",
        "\n",
        "        for batch_idx, content_images in enumerate(pbar):\n",
        "            content_images = content_images.to(device)\n",
        "\n",
        "            # --- 1. Training step ---\n",
        "            optimizer.zero_grad()\n",
        "            generated_images = model(content_images)\n",
        "            total_loss, c_loss, s_loss, tv_loss = criterion(generated_images, content_images)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            current_step += 1\n",
        "            loss_history.append(total_loss.item())\n",
        "\n",
        "            # Log training loss\n",
        "            writer.add_scalar('loss/train/total', total_loss.item(), current_step)\n",
        "            writer.add_scalar('loss/train/content', c_loss.item(), current_step)\n",
        "            writer.add_scalar('loss/train/style', s_loss.item(), current_step)\n",
        "\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'Loss': f\"{total_loss.item():.2e}\",\n",
        "                'LR': f\"{optimizer.param_groups[0]['lr']:.2e}\"\n",
        "            })\n",
        "\n",
        "            # --- 2. Validation step ---\n",
        "            if val_loader and current_step % val_interval == 0:\n",
        "                model.eval() # Switch to eval mode\n",
        "\n",
        "                val_loss_total = 0.0\n",
        "                val_loss_c = 0.0\n",
        "                val_loss_s = 0.0\n",
        "                run_batches = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for i, val_imgs in enumerate(val_loader):\n",
        "                        if i >= val_batches:\n",
        "                            break\n",
        "\n",
        "                        val_imgs = val_imgs.to(device)\n",
        "                        val_gen = model(val_imgs)\n",
        "                        v_total, v_c, v_s, _ = criterion(val_gen, val_imgs)\n",
        "\n",
        "                        val_loss_total += v_total.item()\n",
        "                        val_loss_c += v_c.item()\n",
        "                        val_loss_s += v_s.item()\n",
        "\n",
        "                        run_batches += 1\n",
        "\n",
        "                if run_batches > 0:\n",
        "                    avg_v_total = val_loss_total / run_batches\n",
        "                    avg_v_c = val_loss_c / run_batches\n",
        "                    avg_v_s = val_loss_s / run_batches\n",
        "\n",
        "                    writer.add_scalar('loss/val/total', avg_v_total, current_step)\n",
        "                    writer.add_scalar('loss/val/content', avg_v_c, current_step)\n",
        "                    writer.add_scalar('loss/val/style', avg_v_s, current_step)\n",
        "\n",
        "                    # Step the scheduler\n",
        "                    # Feed the average validation total loss to the scheduler\n",
        "                    # If that loss fails to drop for 'patience' validations, the LR will be reduced\n",
        "                    scheduler.step(avg_v_total)\n",
        "                    # Log the current learning rate\n",
        "                    current_lr = optimizer.param_groups[0]['lr']\n",
        "                    writer.add_scalar('train/learning_rate', current_lr, current_step)\n",
        "\n",
        "                # Visualize images\n",
        "                val_gen_fixed = model(fixed_content_image)\n",
        "                vis_gen = denormalize_img(val_gen_fixed)\n",
        "                writer.add_image('images/generated', vis_gen, current_step, dataformats='HWC')\n",
        "\n",
        "                # Switch back to training mode\n",
        "                model.train()\n",
        "\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "    save_path = f\"{save_dir}/{model_name}.pth\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "\n",
        "    writer.close()\n",
        "    del criterion\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return loss_history, duration"
      ],
      "metadata": {
        "id": "jbtW4zRmynRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. Run training (Baseline vs. Lightweight)\n",
        "# ==========================================\n",
        "\n",
        "# These parameters (epochs, weights) are empirical defaults\n",
        "EPOCHS = 1\n",
        "CONTENT_WEIGHT = 1e5\n",
        "STYLE_WEIGHT = 1e10\n",
        "TV_WEIGHT = 1e-6  # Add a bit of TV loss to smooth the image\n",
        "SCHEDULER_PATIENCE=3\n",
        "SCHEDULER_FACTOR=0.9\n",
        "VAL_INTERVAL=200\n",
        "VAL_BATCH=1\n",
        "\n",
        "print(\">>> Sampling a fixed validation image for visualization...\")\n",
        "# Grab one batch from val_loader\n",
        "val_iter = iter(val_loader)\n",
        "fixed_val_batch = next(val_iter)\n",
        "# Take the first image, add batch dim (1, 3, H, W), and move to GPU\n",
        "fixed_val_image = fixed_val_batch[0].unsqueeze(0).to(device)\n",
        "print(f\"âœ… Locked in one validation image, shape: {fixed_val_image.shape}\")\n",
        "\n",
        "\n",
        "print(\"\\n>>> Ready to start the comparison experiments...\")\n",
        "\n",
        "# Launch TensorBoard and point it to the log directory above\n",
        "!rm -rf ./runs\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "b0aUyL1YkqyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Experiment 1: train the Baseline model ---\n",
        "baseline = BaselineTransformerNet()\n",
        "loss_history_base, time_base = train_style_transfer(\n",
        "    baseline,\n",
        "    \"baseline\",\n",
        "    train_loader,\n",
        "    style_img_tensor,\n",
        "    fixed_val_image,  # pass in the fixed image\n",
        "    SCHEDULER_PATIENCE,\n",
        "    scheduler_factor=SCHEDULER_FACTOR,\n",
        "    val_loader=val_loader,     # pass in the validation set\n",
        "    val_interval=VAL_INTERVAL,\n",
        "    val_batches=VAL_BATCH,\n",
        "    epochs=EPOCHS,\n",
        "    content_weight=CONTENT_WEIGHT,\n",
        "    style_weight=STYLE_WEIGHT,\n",
        "    tv_weight=TV_WEIGHT\n",
        ")"
      ],
      "metadata": {
        "id": "GSOMciYVM1Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Experiment 2: train the Lightweight - V1 model ---\n",
        "v1 = LightweightTransformerNet(\"v1\")\n",
        "loss_history_light, time_light = train_style_transfer(\n",
        "    v1,\n",
        "    \"lightweight-v1\",\n",
        "    train_loader,\n",
        "    style_img_tensor,\n",
        "    fixed_val_image,  # pass in the same fixed image\n",
        "    scheduler_patience=SCHEDULER_PATIENCE,\n",
        "    scheduler_factor=SCHEDULER_FACTOR,\n",
        "    val_loader=val_loader,     # pass in the validation set\n",
        "    val_interval=VAL_INTERVAL,\n",
        "    val_batches=VAL_BATCH,\n",
        "    epochs=EPOCHS,\n",
        "    content_weight=CONTENT_WEIGHT,\n",
        "    style_weight=STYLE_WEIGHT,\n",
        "    tv_weight=TV_WEIGHT\n",
        ")"
      ],
      "metadata": {
        "id": "wIVN-qqZCVm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Experiment 3: train the Lightweight - V2 model ---\n",
        "v2 = LightweightTransformerNet(\"v2\")\n",
        "loss_history_light, time_light = train_style_transfer(\n",
        "    v2,\n",
        "    \"lightweight-v2\",\n",
        "    train_loader,\n",
        "    style_img_tensor,\n",
        "    fixed_val_image,  # pass in the same fixed image\n",
        "    scheduler_patience=SCHEDULER_PATIENCE,\n",
        "    scheduler_factor=SCHEDULER_FACTOR,\n",
        "    val_loader=val_loader,     # pass in the validation set\n",
        "    val_interval=VAL_INTERVAL,\n",
        "    val_batches=VAL_BATCH,\n",
        "    epochs=EPOCHS,\n",
        "    content_weight=CONTENT_WEIGHT,\n",
        "    style_weight=STYLE_WEIGHT,\n",
        "    tv_weight=TV_WEIGHT\n",
        ")"
      ],
      "metadata": {
        "id": "NbdITvxmNAJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Experiment 4: train the Lightweight - V3 model ---\n",
        "v3 = LightweightTransformerNet(\"v3\")\n",
        "loss_history_light, time_light = train_style_transfer(\n",
        "    v3,\n",
        "    \"lightweight-v3\",\n",
        "    train_loader,\n",
        "    style_img_tensor,\n",
        "    fixed_val_image,  # pass in the same fixed image\n",
        "    scheduler_patience=SCHEDULER_PATIENCE,\n",
        "    scheduler_factor=SCHEDULER_FACTOR,\n",
        "    val_loader=val_loader,     # pass in the validation set\n",
        "    val_interval=VAL_INTERVAL,\n",
        "    val_batches=VAL_BATCH,\n",
        "    epochs=EPOCHS,\n",
        "    content_weight=CONTENT_WEIGHT,\n",
        "    style_weight=STYLE_WEIGHT,\n",
        "    tv_weight=TV_WEIGHT\n",
        ")"
      ],
      "metadata": {
        "id": "mm80jY1eNEaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "import time\n",
        "from google.colab import files\n",
        "\n",
        "# Generate a readable timestamp like 2025_01_31_14_23_50\n",
        "timestamp = time.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
        "archive_name = f\"{timestamp}.zip\"\n",
        "\n",
        "# Create a temporary directory\n",
        "temp_dir = \"temp_pack\"\n",
        "os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "# Copy the folders to be packaged\n",
        "shutil.copytree(\"checkpoints\", os.path.join(temp_dir, \"checkpoints\"))\n",
        "shutil.copytree(\"runs\", os.path.join(temp_dir, \"runs\"))\n",
        "\n",
        "# Archive\n",
        "shutil.make_archive(timestamp, \"zip\", temp_dir)\n",
        "\n",
        "# Download\n",
        "files.download(archive_name)\n",
        "\n",
        "# Clean up\n",
        "shutil.rmtree(temp_dir)"
      ],
      "metadata": {
        "id": "hZjpdHrijxuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_feature_maps(feature_maps, per_row=8):\n",
        "    \"\"\"\n",
        "    feature_maps: dict where values are shaped [B, C, H, W]\n",
        "    per_row: max number of channels per row (default eight)\n",
        "    \"\"\"\n",
        "    for layer_name, fmap in feature_maps.items():\n",
        "        fmap = fmap[0].cpu()\n",
        "        total_channels = fmap.shape[0]\n",
        "\n",
        "        cols = per_row\n",
        "        rows = math.ceil(total_channels / cols)\n",
        "\n",
        "        fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
        "\n",
        "        # Ensure axes is a numpy array and reshape safely\n",
        "        if isinstance(axes, np.ndarray):\n",
        "            axes = axes.reshape(rows, cols)\n",
        "        else:\n",
        "            axes = np.array([[axes]])\n",
        "\n",
        "        ch = 0\n",
        "        for r in range(rows):\n",
        "            for c in range(cols):\n",
        "                ax = axes[r][c]\n",
        "                if ch < total_channels:\n",
        "                    channel = fmap[ch].numpy()\n",
        "                    channel = (channel - channel.min()) / (channel.max() - channel.min() + 1e-5)\n",
        "                    ax.imshow(channel, cmap=\"gray\")\n",
        "                    ax.set_title(f\"ch {ch}\", fontsize=8)\n",
        "                    ax.axis(\"off\")\n",
        "                else:\n",
        "                    ax.axis(\"off\")\n",
        "                ch += 1\n",
        "\n",
        "        plt.suptitle(f\"Feature maps of layer {layer_name}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def visualize(model, image, *layers_to_watch):\n",
        "    # Register hooks\n",
        "    feature_maps, hooks = register_feature_hooks(model, layers_to_watch)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    try:\n",
        "        # Get the device of the first parameter of the model\n",
        "        model_device = next(model.parameters()).device\n",
        "    except StopIteration:\n",
        "        # Fallback if model has no parameters (unlikely)\n",
        "        model_device = \"cpu\"\n",
        "\n",
        "    # Run a forward pass to get intermediate outputs\n",
        "    with torch.no_grad():\n",
        "        # Ensure input image is on the same device as the model\n",
        "        _ = model(image.to(model_device))\n",
        "\n",
        "    # Remove the hooks afterward to avoid side effects\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "    # Visualization\n",
        "    show_feature_maps(feature_maps, per_row=6)\n",
        "\n",
        "visualize(v2, fixed_val_image, \"conv1\")"
      ],
      "metadata": {
        "id": "2aw2HPjevk_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = fixed_val_image.to(\"cpu\")\n",
        "\n",
        "\n",
        "def bench(model, name, loops=50):\n",
        "        model = model.to(\"cpu\")\n",
        "        model.eval()\n",
        "        x = input_tensor\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(5):\n",
        "                _ = model(x)\n",
        "\n",
        "        start = time.time()\n",
        "        with torch.no_grad():\n",
        "            for _ in range(loops):\n",
        "                _ = model(x)\n",
        "        end = time.time()\n",
        "\n",
        "        t = (end - start) / loops\n",
        "        print(f\"{name} average latency {t*1000:.2f} ms\")\n",
        "        return t\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"â±ï¸ CPU inference speed comparison\")\n",
        "print(\"-\" * 60)\n",
        "t0 = bench(baseline, \"Baseline\")\n",
        "print(\"-\" * 60)\n",
        "t1 = bench(v1, \"v1\")\n",
        "print(f\"ðŸš€ lightweight-v1 speedup {t0/t1:.2f}x\")\n",
        "print(\"-\" * 60)\n",
        "t2 = bench(v2, \"v2\")\n",
        "print(f\"ðŸš€ lightweight-v2 speedup {t0/t2:.2f}x\")\n",
        "t3 = bench(v3, \"v3\")\n",
        "print(f\"ðŸš€ lightweight-v3 speedup {t0/t3:.2f}x\")\n",
        "print(\"-\" * 60)\n"
      ],
      "metadata": {
        "id": "RpCDJUOTRRsq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}