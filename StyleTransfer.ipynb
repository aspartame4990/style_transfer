{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "low4ol2YQXRk"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboardX\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm  # Use the Notebook-friendly progress bar\n",
        "from tensorboardX import SummaryWriter\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from io import BytesIO\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.models as models\n",
        "from torchvision.models import VGG16_Weights\n",
        "import random\n",
        "\n",
        "\n",
        "device=\"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(f\"âœ… Successfully connected to GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# 2. Define download paths\n",
        "data_dir = Path('./data/coco')\n",
        "data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 3. Download COCO 2017 Validation Set (as our training material)\n",
        "# This is a compromise: the full Train set is huge; the Val set has 5000 images and is enough for the Style Transfer demo\n",
        "coco_url = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
        "zip_path = data_dir / \"val2017.zip\"\n",
        "\n",
        "if not (data_dir / \"val2017\").exists():\n",
        "    print(\"â¬‡ï¸ Downloading the COCO dataset (~1GB), please wait...\")\n",
        "    !wget -q -P {data_dir} {coco_url}\n",
        "\n",
        "    print(\"ðŸ“¦ Extracting...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "\n",
        "    # Delete the zip to save space\n",
        "    os.remove(zip_path)\n",
        "    print(\"âœ… Dataset is ready!\")\n",
        "else:\n",
        "    print(\"âœ… Dataset already exists, skipping download.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M2P6douQcX1"
      },
      "outputs": [],
      "source": [
        "# 1. Collect all image paths\n",
        "image_dir = data_dir / \"val2017\"\n",
        "all_image_paths = sorted(list(image_dir.glob(\"*.jpg\")))\n",
        "print(f\"Total number of images: {len(all_image_paths)}\")\n",
        "\n",
        "# 2. Split the dataset\n",
        "# First separate a test set (e.g., reserve 50 images for the final demo)\n",
        "train_val_paths, test_paths = train_test_split(all_image_paths, test_size=50, random_state=42)\n",
        "\n",
        "# Then split the rest into Train and Validation (e.g., 9:1)\n",
        "train_paths, val_paths = train_test_split(train_val_paths, test_size=0.1, random_state=42)\n",
        "\n",
        "print(f\"--- Dataset split details (record these numbers in the report) ---\")\n",
        "print(f\"ðŸ”¹ Training set (for training): {len(train_paths)} images\")\n",
        "print(f\"ðŸ”¸ Validation set (for plotting the loss curve): {len(val_paths)} images\")\n",
        "print(f\"ðŸš€ Test set (for the final showcase): {len(test_paths)} images\")\n",
        "\n",
        "\n",
        "# 3. Define the PyTorch Dataset class\n",
        "class ContentDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {img_path}: {e}\")\n",
        "            return torch.zeros(3, 1, 1)  # Return an all-black image to avoid crashes\n",
        "\n",
        "\n",
        "# 4. Define preprocessing (Transforms)\n",
        "# Style transfer usually resizes images to a fixed size (e.g., 256x256) and normalizes them\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0.0, std=0.05):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        # tensor is a [C, H, W] tensor and stays within 0 to 1\n",
        "        noise = torch.randn_like(tensor) * self.std + self.mean\n",
        "        noised = tensor + noise\n",
        "        # Keep it within the 0 to 1 range\n",
        "        return torch.clamp(noised, 0.0, 1.0)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}(mean={self.mean}, std={self.std})\"\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(256),\n",
        "    transforms.ToTensor(),\n",
        "    AddGaussianNoise(mean=0.0, std=0.05),\n",
        "])\n",
        "\n",
        "# 5. Create DataLoaders\n",
        "batch_size = 4  # Adjust based on Colab memory; if OOM, change to 2 or 1\n",
        "\n",
        "train_dataset = ContentDataset(train_paths, transform=train_transform)\n",
        "val_dataset = ContentDataset(val_paths, transform=train_transform)\n",
        "# Test dataset usually doesn't need a DataLoader; just read single images for display\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"âœ… DataLoaders are ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-rwOlEnemKV"
      },
      "outputs": [],
      "source": [
        "# 1. Define the download function\n",
        "def load_image_from_url(url, target_size=None):\n",
        "    # Pretend to be a browser to avoid being blocked\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()  # Check for 403/404 errors\n",
        "\n",
        "        img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "\n",
        "        if target_size:\n",
        "            img = img.resize(target_size, Image.BICUBIC)\n",
        "        return img\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Download failed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# 2. Download 'Starry Night'\n",
        "# Backup URL (if Wikimedia still fails, use this fallback)\n",
        "style_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\"\n",
        "style_image_name = \"starry_night.jpg\"\n",
        "\n",
        "print(f\"â¬‡ï¸ Downloading style image: {style_image_name}...\")\n",
        "style_img_pil = load_image_from_url(style_url, target_size=(256, 256))\n",
        "\n",
        "if style_img_pil is not None:\n",
        "    style_img_pil.save(style_image_name)\n",
        "\n",
        "    # 3. Preprocess the style image\n",
        "    style_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Add the batch dimension\n",
        "    style_img_tensor = style_transform(style_img_pil).unsqueeze(0).to(device)\n",
        "    print(f\"âœ… Style image ready, tensor shape: {style_img_tensor.shape}\")\n",
        "\n",
        "    # 4. Visualize to verify\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(style_img_pil)\n",
        "    plt.title(\"Style Image: Starry Night\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"âš ï¸ Image download failed. Try another URL or manually upload 'starry_night.jpg' to the Colab file panel.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iW-EvH5gepuC"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 1. Basic components\n",
        "# ==========================================\n",
        "\n",
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.reflection_pad(x)\n",
        "        out = self.conv2d(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class LightweightConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        super(LightweightConvLayer, self).__init__()\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
        "\n",
        "        # 1. Depthwise convolution: groups=in_channels, extracts spatial features\n",
        "        # Followed by Norm, so bias=False\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size,\n",
        "                                   stride=stride, groups=in_channels, bias=False)\n",
        "\n",
        "        # [New] Insert Norm and Activation between them to increase nonlinearity\n",
        "        self.dw_norm = nn.InstanceNorm2d(in_channels, affine=True)\n",
        "        self.dw_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # 2. Pointwise convolution: 1x1 conv to combine channel features\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                                   stride=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.reflection_pad(x)\n",
        "\n",
        "        # Depthwise stage\n",
        "        out = self.depthwise(out)\n",
        "        out = self.dw_norm(out)\n",
        "        out = self.dw_relu(out)\n",
        "\n",
        "        # Pointwise stage\n",
        "        out = self.pointwise(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 2. Residual block\n",
        "# ==========================================\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Standard Residual Block\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in1 = nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in2 = nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.in1(self.conv1(x)))\n",
        "        out = self.in2(self.conv2(out))\n",
        "        out = out + residual\n",
        "        return out\n",
        "\n",
        "\n",
        "class LightweightResidualBlock(nn.Module):\n",
        "    \"\"\"Lightweight Residual Block (Depthwise Separable)\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super(LightweightResidualBlock, self).__init__()\n",
        "        # Replace the standard ConvLayer with LightweightConvLayer\n",
        "        self.conv1 = LightweightConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in1 = nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.conv2 = LightweightConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in2 = nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.in1(self.conv1(x)))\n",
        "        out = self.in2(self.conv2(out))\n",
        "        out = out + residual\n",
        "        return out\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 3. Upsampling layer\n",
        "# ==========================================\n",
        "\n",
        "class UpsampleConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
        "        super(UpsampleConvLayer, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_in = x\n",
        "        if self.upsample:\n",
        "            x_in = torch.nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n",
        "        out = self.reflection_pad(x_in)\n",
        "        out = self.conv2d(out)\n",
        "        return out\n",
        "\n",
        "class LightweightUpsampleConvLayer(nn.Module):\n",
        "    \"\"\"Lightweight upsampling layer: Upsample + Lightweight Conv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
        "        super(LightweightUpsampleConvLayer, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        # Reuse LightweightConvLayer\n",
        "        self.conv = LightweightConvLayer(in_channels, out_channels, kernel_size, stride)\n",
        "    def forward(self, x):\n",
        "        x_in = x\n",
        "        if self.upsample:\n",
        "            x_in = torch.nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n",
        "        out = self.conv(x_in)\n",
        "        return out\n",
        "\n",
        "# ==========================================\n",
        "# 4. Model definition\n",
        "# ==========================================\n",
        "\n",
        "class BaselineTransformerNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaselineTransformerNet, self).__init__()\n",
        "        # Initial convolution layers\n",
        "        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n",
        "        self.in1 = nn.InstanceNorm2d(32, affine=True)\n",
        "        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
        "        self.in2 = nn.InstanceNorm2d(64, affine=True)\n",
        "        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n",
        "        self.in3 = nn.InstanceNorm2d(128, affine=True)\n",
        "\n",
        "        # Residual layers (Standard)\n",
        "        self.res1 = ResidualBlock(128)\n",
        "        self.res2 = ResidualBlock(128)\n",
        "        self.res3 = ResidualBlock(128)\n",
        "        self.res4 = ResidualBlock(128)\n",
        "        self.res5 = ResidualBlock(128)\n",
        "\n",
        "        # Upsampling Layers\n",
        "        self.deconv1 = UpsampleConvLayer(128, 64, kernel_size=3, stride=1, upsample=2)\n",
        "        self.in4 = nn.InstanceNorm2d(64, affine=True)\n",
        "        self.deconv2 = UpsampleConvLayer(64, 32, kernel_size=3, stride=1, upsample=2)\n",
        "        self.in5 = nn.InstanceNorm2d(32, affine=True)\n",
        "        self.deconv3 = ConvLayer(32, 3, kernel_size=9, stride=1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, X):\n",
        "        y = self.relu(self.in1(self.conv1(X)))\n",
        "        y = self.relu(self.in2(self.conv2(y)))\n",
        "        y = self.relu(self.in3(self.conv3(y)))\n",
        "        y = self.res1(y)\n",
        "        y = self.res2(y)\n",
        "        y = self.res3(y)\n",
        "        y = self.res4(y)\n",
        "        y = self.res5(y)\n",
        "        y = self.relu(self.in4(self.deconv1(y)))\n",
        "        y = self.relu(self.in5(self.deconv2(y)))\n",
        "        y = self.deconv3(y)\n",
        "        return torch.sigmoid(y)\n",
        "\n",
        "class LightweightTransformerNet(nn.Module):\n",
        "    def __init__(self, mode='v1'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            mode (str):\n",
        "                'v1' - Only Residual Blocks use lightweight conv (Baseline Encoder/Decoder)\n",
        "                'v2' - Encoder + Residual Blocks use lightweight conv\n",
        "                'v3' - Fully lightweight (Encoder + Res + Decoder)\n",
        "        \"\"\"\n",
        "        super(LightweightTransformerNet, self).__init__()\n",
        "        self.mode = mode\n",
        "        print(f\"ðŸ—ï¸ Initializing Lightweight Model with mode: {mode}\")\n",
        "\n",
        "        # 1. Dynamically choose layer types (Dynamic Layer Selection)\n",
        "        # ---------------------------------------------------\n",
        "        # Encoder Layer Type\n",
        "        if mode in ['v2', 'v3']:\n",
        "            EncLayer = LightweightConvLayer\n",
        "        else:\n",
        "            EncLayer = ConvLayer  # v1 uses standard conv as the encoder\n",
        "\n",
        "        # Decoder Layer Type\n",
        "        if mode == 'v3':\n",
        "            UpLayer = LightweightUpsampleConvLayer\n",
        "            FinalLayer = LightweightConvLayer\n",
        "        else:\n",
        "            UpLayer = UpsampleConvLayer # v1, v2 use standard conv as the decoder\n",
        "            FinalLayer = ConvLayer\n",
        "\n",
        "        # ---------------------------------------------------\n",
        "\n",
        "        # Initial convolution layers (Encoder)\n",
        "        self.conv1 = EncLayer(3, 32, kernel_size=9, stride=1)\n",
        "        self.in1 = nn.InstanceNorm2d(32, affine=True)\n",
        "        self.conv2 = EncLayer(32, 64, kernel_size=3, stride=2)\n",
        "        self.in2 = nn.InstanceNorm2d(64, affine=True)\n",
        "        self.conv3 = EncLayer(64, 128, kernel_size=3, stride=2)\n",
        "        self.in3 = nn.InstanceNorm2d(128, affine=True)\n",
        "\n",
        "        # Residual layers\n",
        "        self.res1 = LightweightResidualBlock(128)\n",
        "        self.res2 = LightweightResidualBlock(128)\n",
        "        self.res3 = LightweightResidualBlock(128)\n",
        "        self.res4 = LightweightResidualBlock(128)\n",
        "        self.res5 = LightweightResidualBlock(128)\n",
        "\n",
        "        # Upsampling Layers (Decoder)\n",
        "        self.deconv1 = UpLayer(128, 64, kernel_size=3, stride=1, upsample=2)\n",
        "        self.in4 = nn.InstanceNorm2d(64, affine=True)\n",
        "        self.deconv2 = UpLayer(64, 32, kernel_size=3, stride=1, upsample=2)\n",
        "        self.in5 = nn.InstanceNorm2d(32, affine=True)\n",
        "\n",
        "        # Final Output Layer\n",
        "        self.deconv3 = FinalLayer(32, 3, kernel_size=9, stride=1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, X):\n",
        "        y = self.relu(self.in1(self.conv1(X)))\n",
        "        y = self.relu(self.in2(self.conv2(y)))\n",
        "        y = self.relu(self.in3(self.conv3(y)))\n",
        "        y = self.res1(y)\n",
        "        y = self.res2(y)\n",
        "        y = self.res3(y)\n",
        "        y = self.res4(y)\n",
        "        y = self.res5(y)\n",
        "        y = self.relu(self.in4(self.deconv1(y)))\n",
        "        y = self.relu(self.in5(self.deconv2(y)))\n",
        "        y = self.deconv3(y)\n",
        "        return torch.sigmoid(y)\n",
        "\n",
        "# ==========================================\n",
        "# Visualization\n",
        "# ==========================================\n",
        "\n",
        "def register_feature_hooks(model, layer_names):\n",
        "    \"\"\"\n",
        "    model: any nn.Module\n",
        "    layer_names: list of layer names to inspect, e.g., [\"conv1\", \"res1\", \"deconv1\"]\n",
        "    Returns: feature_maps dict and hooks list\n",
        "    \"\"\"\n",
        "    feature_maps = {}\n",
        "\n",
        "    def get_hook(name):\n",
        "        def hook(module, input, output):\n",
        "            # Save a copy to the CPU for visualization\n",
        "            feature_maps[name] = output.detach().cpu()\n",
        "        return hook\n",
        "\n",
        "    hooks = []\n",
        "    for name, module in model.named_modules():\n",
        "        if name in layer_names:\n",
        "            hooks.append(module.register_forward_hook(get_hook(name)))\n",
        "    return feature_maps, hooks\n",
        "\n",
        "# ==========================================\n",
        "# Verify parameter counts\n",
        "# ==========================================\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "baseline = BaselineTransformerNet()\n",
        "v1 = LightweightTransformerNet(\"v1\")\n",
        "v2 = LightweightTransformerNet(\"v2\")\n",
        "v3 = LightweightTransformerNet(\"v3\")\n",
        "\n",
        "params_baseline = count_parameters(baseline)\n",
        "params_v1 = count_parameters(v1)\n",
        "params_v2 = count_parameters(v2)\n",
        "params_v3 = count_parameters(v3)\n",
        "\n",
        "print(f\"ðŸ“Š Baseline Model Parameters: {params_baseline:,}\")\n",
        "print(f\"ðŸš€ Lightweight-V1 Model Parameters: {params_v1:,}\")\n",
        "print(f\"ðŸ“‰ Reduction: {(1 - params_v1 / params_baseline) * 100:.2f}%\")\n",
        "print(f\"ðŸš€ Lightweight-V2 Model Parameters: {params_v2:,}\")\n",
        "print(f\"ðŸ“‰ Reduction: {(1 - params_v2 / params_baseline) * 100:.2f}%\")\n",
        "print(f\"ðŸš€ Lightweight-V3 Model Parameters: {params_v3:,}\")\n",
        "print(f\"ðŸ“‰ Reduction: {(1 - params_v3 / params_baseline) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXhRJgxFBqQO"
      },
      "outputs": [],
      "source": [
        "def denormalize_img(tensor):\n",
        "    \"\"\"\n",
        "    Input: Tensor (B, C, H, W) or (C, H, W) range [0, 1]\n",
        "    Output: Numpy Array (H, W, C) range [0, 255]\n",
        "    \"\"\"\n",
        "    img = tensor.clone().detach().cpu()\n",
        "\n",
        "    if img.dim() == 4:\n",
        "        img = img[0]\n",
        "\n",
        "    img = img.permute(1, 2, 0).numpy()\n",
        "\n",
        "    # Only need to clip for safety; mean/std computation is no longer required\n",
        "    img = np.clip(img, 0, 1)\n",
        "    img = (img * 255).astype(np.uint8)\n",
        "\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hF1mCfqapn9B"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(input):\n",
        "    batch_size, c, h, w = input.size()\n",
        "    features = input.view(batch_size, c, h * w)\n",
        "    G = torch.bmm(features, features.transpose(1, 2))\n",
        "    return G.div(c * h * w)\n",
        "\n",
        "class Vgg16Features(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Vgg16Features, self).__init__()\n",
        "        vgg16 = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
        "        self.features = vgg16.features.eval()\n",
        "        for param in self.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Define the normalization parameters required by VGG\n",
        "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
        "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Manually normalize before feeding into VGG\n",
        "        # x is assumed to be in [0, 1]\n",
        "        x = (x - self.mean) / self.std\n",
        "\n",
        "        features = []\n",
        "        style_layers_indices = {3, 8, 15, 22}\n",
        "        for name, layer in self.features._modules.items():\n",
        "            x = layer(x)\n",
        "            if int(name) in style_layers_indices:\n",
        "                features.append(x)\n",
        "        return features\n",
        "\n",
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self, style_img_tensor, content_weight=1e5, style_weight=1e10, tv_weight=0):\n",
        "        super(PerceptualLoss, self).__init__()\n",
        "        self.vgg = Vgg16Features().to(device)\n",
        "        self.content_weight = content_weight\n",
        "        self.style_weight = style_weight\n",
        "        self.tv_weight = tv_weight\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "        # Compute style Grams (style_img_tensor is now in 0-1; VGG will normalize internally)\n",
        "        with torch.no_grad():\n",
        "            style_img_tensor = style_img_tensor.to(device)\n",
        "            self.style_features = self.vgg(style_img_tensor)\n",
        "            self.style_grams = [gram_matrix(f).detach() for f in self.style_features]\n",
        "\n",
        "    def total_variation_loss(self, img):\n",
        "        bs_img, c_img, h_img, w_img = img.size()\n",
        "        tv_h = torch.pow(img[:, :, 1:, :] - img[:, :, :-1, :], 2).sum()\n",
        "        tv_w = torch.pow(img[:, :, :, 1:] - img[:, :, :, :-1], 2).sum()\n",
        "        return (tv_h + tv_w) / (bs_img * c_img * h_img * w_img)\n",
        "\n",
        "    def forward(self, generated_img, content_img):\n",
        "        # generated_img and content_img are both in [0, 1]\n",
        "        current_batch_size = generated_img.shape[0]\n",
        "\n",
        "        gen_features = self.vgg(generated_img)\n",
        "        content_features = self.vgg(content_img)\n",
        "\n",
        "        # Content Loss (commonly relu2_2, i.e., index 1 in the list)\n",
        "        content_loss = self.content_weight * self.mse_loss(gen_features[1], content_features[1])\n",
        "\n",
        "        # Style Loss\n",
        "        style_loss = 0.0\n",
        "        for gen_feat, target_gram in zip(gen_features, self.style_grams):\n",
        "            gen_gram = gram_matrix(gen_feat)\n",
        "            target_gram_expanded = target_gram.expand(current_batch_size, -1, -1)\n",
        "            style_loss += self.mse_loss(gen_gram, target_gram_expanded)\n",
        "\n",
        "        style_loss *= self.style_weight\n",
        "\n",
        "        # Total Variation Loss\n",
        "        tv_loss = self.tv_weight * self.total_variation_loss(generated_img)\n",
        "\n",
        "        total_loss = content_loss + style_loss + tv_loss\n",
        "\n",
        "        return total_loss, content_loss, style_loss, tv_loss\n",
        "\n",
        "print(\"âœ… Loss function updated with normalization logic!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbtW4zRmynRd"
      },
      "outputs": [],
      "source": [
        "def train_style_transfer(model, model_name, train_loader, style_img_tensor,\n",
        "                         fixed_content_image,\n",
        "                         scheduler_patience,\n",
        "                         scheduler_factor,\n",
        "                         val_loader,\n",
        "                         val_interval,\n",
        "                         val_batches,\n",
        "                         epochs=2,\n",
        "                         lr=1e-3,\n",
        "                         content_weight=1e5,\n",
        "                         style_weight=1e10,\n",
        "                         tv_weight=0,\n",
        "                         save_dir='./checkpoints'\n",
        "                         ):\n",
        "\n",
        "    import os\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Initialize ReduceLROnPlateau scheduler\n",
        "    # mode='min': triggers when the monitored metric (loss) stops decreasing\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                  factor=scheduler_factor,\n",
        "                                  patience=scheduler_patience\n",
        "                                  )\n",
        "\n",
        "    criterion = PerceptualLoss(style_img_tensor, content_weight, style_weight, tv_weight)\n",
        "\n",
        "    log_dir = os.path.join(\"runs\", model_name)\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "    print(f\"ðŸ“ TensorBoard log directory: {log_dir}\")\n",
        "\n",
        "    loss_history = []\n",
        "    start_time = time.time()\n",
        "    current_step = 0\n",
        "\n",
        "    print(f\"ðŸš€ Starting training: {model_name} | Epochs: {epochs}\")\n",
        "\n",
        "    # Log the original image first\n",
        "    writer.add_image('images/content', denormalize_img(fixed_content_image), dataformats='HWC')\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\")\n",
        "\n",
        "        for batch_idx, content_images in enumerate(pbar):\n",
        "            content_images = content_images.to(device)\n",
        "\n",
        "            # --- 1. Training step ---\n",
        "            optimizer.zero_grad()\n",
        "            generated_images = model(content_images)\n",
        "            total_loss, c_loss, s_loss, tv_loss = criterion(generated_images, content_images)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            current_step += 1\n",
        "            loss_history.append(total_loss.item())\n",
        "\n",
        "            # Log training loss\n",
        "            writer.add_scalar('loss/train/total', total_loss.item(), current_step)\n",
        "            writer.add_scalar('loss/train/content', c_loss.item(), current_step)\n",
        "            writer.add_scalar('loss/train/style', s_loss.item(), current_step)\n",
        "\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'Loss': f\"{total_loss.item():.2e}\",\n",
        "                'LR': f\"{optimizer.param_groups[0]['lr']:.2e}\"\n",
        "            })\n",
        "\n",
        "            # --- 2. Validation step ---\n",
        "            if val_loader and current_step % val_interval == 0:\n",
        "                model.eval() # Switch to eval mode\n",
        "\n",
        "                val_loss_total = 0.0\n",
        "                val_loss_c = 0.0\n",
        "                val_loss_s = 0.0\n",
        "                run_batches = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for i, val_imgs in enumerate(val_loader):\n",
        "                        if i >= val_batches:\n",
        "                            break\n",
        "\n",
        "                        val_imgs = val_imgs.to(device)\n",
        "                        val_gen = model(val_imgs)\n",
        "                        v_total, v_c, v_s, _ = criterion(val_gen, val_imgs)\n",
        "\n",
        "                        val_loss_total += v_total.item()\n",
        "                        val_loss_c += v_c.item()\n",
        "                        val_loss_s += v_s.item()\n",
        "\n",
        "                        run_batches += 1\n",
        "\n",
        "                if run_batches > 0:\n",
        "                    avg_v_total = val_loss_total / run_batches\n",
        "                    avg_v_c = val_loss_c / run_batches\n",
        "                    avg_v_s = val_loss_s / run_batches\n",
        "\n",
        "                    writer.add_scalar('loss/val/total', avg_v_total, current_step)\n",
        "                    writer.add_scalar('loss/val/content', avg_v_c, current_step)\n",
        "                    writer.add_scalar('loss/val/style', avg_v_s, current_step)\n",
        "\n",
        "                    # Step the scheduler\n",
        "                    # Feed the average validation total loss to the scheduler\n",
        "                    # If that loss fails to drop for 'patience' validations, the LR will be reduced\n",
        "                    scheduler.step(avg_v_total)\n",
        "                    # Log the current learning rate\n",
        "                    current_lr = optimizer.param_groups[0]['lr']\n",
        "                    writer.add_scalar('train/learning_rate', current_lr, current_step)\n",
        "\n",
        "                # Visualize images\n",
        "                val_gen_fixed = model(fixed_content_image)\n",
        "                vis_gen = denormalize_img(val_gen_fixed)\n",
        "                writer.add_image('images/generated', vis_gen, current_step, dataformats='HWC')\n",
        "\n",
        "                # Switch back to training mode\n",
        "                model.train()\n",
        "\n",
        "        save_path = f\"{save_dir}/{model_name}_{epoch+1}.pth\"\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "    save_path = f\"{save_dir}/{model_name}.pth\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "\n",
        "    writer.close()\n",
        "    del criterion\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return loss_history, duration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0aUyL1YkqyN"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 2. Run training (Baseline vs. Lightweight)\n",
        "# ==========================================\n",
        "\n",
        "# These parameters (epochs, weights) are empirical defaults\n",
        "EPOCHS = 20\n",
        "CONTENT_WEIGHT = 1e5\n",
        "STYLE_WEIGHT = 1e10\n",
        "TV_WEIGHT = 1e-6  # Add a bit of TV loss to smooth the image\n",
        "SCHEDULER_PATIENCE=3\n",
        "SCHEDULER_FACTOR=0.9\n",
        "VAL_INTERVAL=200\n",
        "VAL_BATCH=1\n",
        "\n",
        "print(\">>> Sampling a fixed validation image for visualization...\")\n",
        "# Grab one batch from val_loader\n",
        "val_iter = iter(val_loader)\n",
        "fixed_val_batch = next(val_iter)\n",
        "# Take the first image, add batch dim (1, 3, H, W), and move to GPU\n",
        "fixed_val_image = fixed_val_batch[0].unsqueeze(0).to(device)\n",
        "print(f\"âœ… Locked in one validation image, shape: {fixed_val_image.shape}\")\n",
        "\n",
        "\n",
        "print(\"\\n>>> Ready to start the comparison experiments...\")\n",
        "\n",
        "# Launch TensorBoard and point it to the log directory above\n",
        "!rm -rf ./runs\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSOMciYVM1Hk"
      },
      "outputs": [],
      "source": [
        "# --- Experiment 1: train the Baseline model ---\n",
        "baseline = BaselineTransformerNet()\n",
        "loss_history_base, time_base = train_style_transfer(\n",
        "    baseline,\n",
        "    \"baseline\",\n",
        "    train_loader,\n",
        "    style_img_tensor,\n",
        "    fixed_val_image,  # pass in the fixed image\n",
        "    SCHEDULER_PATIENCE,\n",
        "    scheduler_factor=SCHEDULER_FACTOR,\n",
        "    val_loader=val_loader,     # pass in the validation set\n",
        "    val_interval=VAL_INTERVAL,\n",
        "    val_batches=VAL_BATCH,\n",
        "    epochs=EPOCHS,\n",
        "    content_weight=CONTENT_WEIGHT,\n",
        "    style_weight=STYLE_WEIGHT,\n",
        "    tv_weight=TV_WEIGHT\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIVN-qqZCVm0"
      },
      "outputs": [],
      "source": [
        "# --- Experiment 2: train the Lightweight - V1 model ---\n",
        "v1 = LightweightTransformerNet(\"v1\")\n",
        "loss_history_light, time_light = train_style_transfer(\n",
        "    v1,\n",
        "    \"lightweight-v1\",\n",
        "    train_loader,\n",
        "    style_img_tensor,\n",
        "    fixed_val_image,  # pass in the same fixed image\n",
        "    scheduler_patience=SCHEDULER_PATIENCE,\n",
        "    scheduler_factor=SCHEDULER_FACTOR,\n",
        "    val_loader=val_loader,     # pass in the validation set\n",
        "    val_interval=VAL_INTERVAL,\n",
        "    val_batches=VAL_BATCH,\n",
        "    epochs=EPOCHS,\n",
        "    content_weight=CONTENT_WEIGHT,\n",
        "    style_weight=STYLE_WEIGHT,\n",
        "    tv_weight=TV_WEIGHT\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbdITvxmNAJs"
      },
      "outputs": [],
      "source": [
        "# --- Experiment 3: train the Lightweight - V2 model ---\n",
        "v2 = LightweightTransformerNet(\"v2\")\n",
        "loss_history_light, time_light = train_style_transfer(\n",
        "    v2,\n",
        "    \"lightweight-v2\",\n",
        "    train_loader,\n",
        "    style_img_tensor,\n",
        "    fixed_val_image,  # pass in the same fixed image\n",
        "    scheduler_patience=SCHEDULER_PATIENCE,\n",
        "    scheduler_factor=SCHEDULER_FACTOR,\n",
        "    val_loader=val_loader,     # pass in the validation set\n",
        "    val_interval=VAL_INTERVAL,\n",
        "    val_batches=VAL_BATCH,\n",
        "    epochs=EPOCHS,\n",
        "    content_weight=CONTENT_WEIGHT,\n",
        "    style_weight=STYLE_WEIGHT,\n",
        "    tv_weight=TV_WEIGHT\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mm80jY1eNEaT"
      },
      "outputs": [],
      "source": [
        "# --- Experiment 4: train the Lightweight - V3 model ---\n",
        "v3 = LightweightTransformerNet(\"v3\")\n",
        "loss_history_light, time_light = train_style_transfer(\n",
        "    v3,\n",
        "    \"lightweight-v3\",\n",
        "    train_loader,\n",
        "    style_img_tensor,\n",
        "    fixed_val_image,  # pass in the same fixed image\n",
        "    scheduler_patience=SCHEDULER_PATIENCE,\n",
        "    scheduler_factor=SCHEDULER_FACTOR,\n",
        "    val_loader=val_loader,     # pass in the validation set\n",
        "    val_interval=VAL_INTERVAL,\n",
        "    val_batches=VAL_BATCH,\n",
        "    epochs=EPOCHS,\n",
        "    content_weight=CONTENT_WEIGHT,\n",
        "    style_weight=STYLE_WEIGHT,\n",
        "    tv_weight=TV_WEIGHT\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZjpdHrijxuZ"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "import time\n",
        "from google.colab import files\n",
        "\n",
        "# Generate a readable timestamp like 2025_01_31_14_23_50\n",
        "timestamp = time.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
        "archive_name = f\"{timestamp}.zip\"\n",
        "\n",
        "# Create a temporary directory\n",
        "temp_dir = \"temp_pack\"\n",
        "os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "# Copy the folders to be packaged\n",
        "shutil.copytree(\"checkpoints\", os.path.join(temp_dir, \"checkpoints\"))\n",
        "shutil.copytree(\"runs\", os.path.join(temp_dir, \"runs\"))\n",
        "\n",
        "# Archive\n",
        "shutil.make_archive(timestamp, \"zip\", temp_dir)\n",
        "\n",
        "# Download\n",
        "files.download(archive_name)\n",
        "\n",
        "# Clean up\n",
        "shutil.rmtree(temp_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ9ykNM4KxJW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import shutil\n",
        "from google.colab import files\n",
        "from torchvision import transforms # Added explicit import if not already in context, but following user prompt assumption\n",
        "\n",
        "# Assuming device, test_paths, BaselineTransformerNet, LightweightTransformerNet\n",
        "# and denormalize_img are already defined in the context or previous code blocks\n",
        "\n",
        "# ==========================\n",
        "# 0  Set Save Path\n",
        "# ==========================\n",
        "output_dir = \"test_output\"\n",
        "!rm -rf ./test_output/\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"Images will be saved to: {output_dir}\")\n",
        "\n",
        "# ==========================\n",
        "# 1  Load Models (Keep Unchanged)\n",
        "# ==========================\n",
        "ckpt_dir = \"./checkpoints\"\n",
        "\n",
        "baseline_eval = BaselineTransformerNet().to(device)\n",
        "baseline_eval.load_state_dict(\n",
        "    torch.load(os.path.join(ckpt_dir, \"baseline.pth\"), map_location=device)\n",
        ")\n",
        "baseline_eval.eval()\n",
        "\n",
        "v1_eval = LightweightTransformerNet(\"v1\").to(device)\n",
        "v1_eval.load_state_dict(\n",
        "    torch.load(os.path.join(ckpt_dir, \"lightweight-v1.pth\"), map_location=device)\n",
        ")\n",
        "v1_eval.eval()\n",
        "\n",
        "v2_eval = LightweightTransformerNet(\"v2\").to(device)\n",
        "v2_eval.load_state_dict(\n",
        "    torch.load(os.path.join(ckpt_dir, \"lightweight-v2.pth\"), map_location=device)\n",
        ")\n",
        "v2_eval.eval()\n",
        "\n",
        "v3_eval = LightweightTransformerNet(\"v3\").to(device)\n",
        "v3_eval.load_state_dict(\n",
        "    torch.load(os.path.join(ckpt_dir, \"lightweight-v3.pth\"), map_location=device)\n",
        ")\n",
        "v3_eval.eval()\n",
        "\n",
        "# ==========================\n",
        "# 2  Define Image Transforms for Test Set\n",
        "# ==========================\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(256),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "def load_content_image(img_path):\n",
        "    img_pil = Image.open(img_path).convert(\"RGB\")\n",
        "    img_tensor = test_transform(img_pil).unsqueeze(0).to(device)\n",
        "    return img_pil, img_tensor\n",
        "\n",
        "# ==========================\n",
        "# 3  Randomly Sample Three Images from Test Set\n",
        "# ==========================\n",
        "num_samples = 3\n",
        "if len(test_paths) < num_samples:\n",
        "    print(f\"Warning: Test set only has {len(test_paths)} images, using all of them.\")\n",
        "    sample_paths = test_paths\n",
        "else:\n",
        "    sample_paths = random.sample(test_paths, num_samples)\n",
        "\n",
        "# ==========================\n",
        "# 4  Style Transfer: Display + Save\n",
        "# ==========================\n",
        "models_for_demo = [\n",
        "    (\"Original\", None),\n",
        "    (\"Baseline\", baseline_eval),\n",
        "    (\"v1\", v1_eval),\n",
        "    (\"v2\", v2_eval),\n",
        "    (\"v3\", v3_eval),\n",
        "]\n",
        "\n",
        "n_rows = len(sample_paths)\n",
        "n_cols = len(models_for_demo)\n",
        "\n",
        "plt.figure(figsize=(4 * n_cols, 4 * n_rows))\n",
        "\n",
        "\n",
        "for row_idx, img_path in enumerate(sample_paths):\n",
        "    content_pil, content_tensor = load_content_image(img_path)\n",
        "\n",
        "    # Get bare filename (e.g., \"dog.jpg\" -> \"dog\")\n",
        "    base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "\n",
        "    for col_idx, (name, model) in enumerate(models_for_demo):\n",
        "        plt_idx = row_idx * n_cols + col_idx + 1\n",
        "        plt.subplot(n_rows, n_cols, plt_idx)\n",
        "\n",
        "        image_to_save = None\n",
        "        save_filename = \"\"\n",
        "\n",
        "        if model is None:\n",
        "            # --- Process Original Image ---\n",
        "            # Resize original image to 256x256 for consistent display and saving\n",
        "            vis_transform = transforms.Compose([\n",
        "               transforms.Resize(256),      # Resize keeping aspect ratio, short edge to 256\n",
        "                transforms.CenterCrop(256)   # Center crop 256x256\n",
        "            ])\n",
        "            resized_pil = vis_transform(content_pil)\n",
        "\n",
        "            # Matplotlib Display\n",
        "            plt.imshow(resized_pil)\n",
        "            plt.title(f\"Original\\n{base_name}\")\n",
        "\n",
        "            # Prepare for saving\n",
        "            image_to_save = resized_pil\n",
        "            save_filename = f\"{base_name}.jpg\"\n",
        "\n",
        "        else:\n",
        "            # --- Process Model Generated Image ---\n",
        "            with torch.no_grad():\n",
        "                out_tensor = model(content_tensor)\n",
        "\n",
        "            # out_img is usually a numpy array (H, W, C)\n",
        "            out_img = denormalize_img(out_tensor)\n",
        "\n",
        "            # Matplotlib Display\n",
        "            plt.imshow(out_img)\n",
        "            plt.title(name)\n",
        "\n",
        "            # Prepare for saving: Convert numpy array to PIL Image\n",
        "            if isinstance(out_img, np.ndarray):\n",
        "                # Ensure uint8 format for saving\n",
        "                if out_img.dtype != np.uint8:\n",
        "                     # Assuming 0-1 range if float; 0-255 if int\n",
        "                    if out_img.max() <= 1.0:\n",
        "                        out_img = (out_img * 255).astype(np.uint8)\n",
        "                    else:\n",
        "                        out_img = out_img.astype(np.uint8)\n",
        "                image_to_save = Image.fromarray(out_img)\n",
        "            else:\n",
        "                # If denormalize_img returns PIL directly\n",
        "                image_to_save = out_img\n",
        "\n",
        "            save_filename = f\"{base_name}_{name}.jpg\"\n",
        "\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        # --- Execute Save Operation ---\n",
        "        if image_to_save is not None:\n",
        "            save_path = os.path.join(output_dir, save_filename)\n",
        "            image_to_save.save(save_path)\n",
        "            # Optional: Print save log\n",
        "            # print(f\"Saved: {save_path}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"All images processed and saved to 'test_output' folder.\")\n",
        "\n",
        "# ==========================\n",
        "# 5  Zip and Download Results\n",
        "# ==========================\n",
        "\n",
        "# Set zip archive name (without extension) and directory to zip\n",
        "zip_name = \"test_results\"\n",
        "target_dir = \"test_output\"  # Corresponds to output_dir defined in previous code\n",
        "\n",
        "print(f\"Zipping '{target_dir}' folder into '{zip_name}.zip' ...\")\n",
        "\n",
        "# 1. Create ZIP archive\n",
        "# shutil.make_archive(archive_name, format, root_dir)\n",
        "shutil.make_archive(zip_name, 'zip', target_dir)\n",
        "\n",
        "# 2. Check if file was generated successfully\n",
        "zip_file_full_path = f\"{zip_name}.zip\"\n",
        "if os.path.exists(zip_file_full_path):\n",
        "    print(f\"Zip successful! File size: {os.path.getsize(zip_file_full_path) / 1024:.2f} KB\")\n",
        "\n",
        "    # 3. Trigger browser download\n",
        "    print(\"Starting download, please check your browser prompts...\")\n",
        "    files.download(zip_file_full_path)\n",
        "else:\n",
        "    print(\"Zip failed, please check if folder path exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpCDJUOTRRsq"
      },
      "outputs": [],
      "source": [
        "input_tensor = fixed_val_image.to(\"cpu\")\n",
        "\n",
        "\n",
        "def bench(model, name, loops=50):\n",
        "        model = model.to(\"cpu\")\n",
        "        model.eval()\n",
        "        x = input_tensor\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(5):\n",
        "                _ = model(x)\n",
        "\n",
        "        start = time.time()\n",
        "        with torch.no_grad():\n",
        "            for _ in range(loops):\n",
        "                _ = model(x)\n",
        "        end = time.time()\n",
        "\n",
        "        t = (end - start) / loops\n",
        "        print(f\"{name} average latency {t*1000:.2f} ms\")\n",
        "        return t\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"â±ï¸ CPU inference speed comparison\")\n",
        "print(\"-\" * 60)\n",
        "t0 = bench(baseline, \"Baseline\")\n",
        "print(\"-\" * 60)\n",
        "t1 = bench(v1, \"v1\")\n",
        "print(f\"ðŸš€ lightweight-v1 speedup {t0/t1:.2f}x\")\n",
        "print(\"-\" * 60)\n",
        "t2 = bench(v2, \"v2\")\n",
        "print(f\"ðŸš€ lightweight-v2 speedup {t0/t2:.2f}x\")\n",
        "print(\"-\" * 60)\n",
        "t3 = bench(v3, \"v3\")\n",
        "print(f\"ðŸš€ lightweight-v3 speedup {t0/t3:.2f}x\")\n",
        "print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rea4IsWpxK6O"
      },
      "outputs": [],
      "source": [
        "# Code Reference Note:\n",
        "#\n",
        "# The classes â€œConvLayerâ€, â€œResidualBlockâ€, â€œUpsampleConvLayerâ€, and â€œBaselineTransformerNetâ€ were adapted from\n",
        "# https://github.com/pytorch/examples/tree/main/fast_neural_style ,\n",
        "# while the remaining parts were generated by LLM or written by the student."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
