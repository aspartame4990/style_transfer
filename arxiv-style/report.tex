\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{cleveref}       % smart cross-referencing
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{caption}
\usepackage{subcaption}

\title{Can depthwise separable convolution make neural style transfer more lightweight? A Comparative Study}

\author{ 
    \href{https://orcid.org/0000-0000-0000-0000}{\includegraphics[scale=0.06]{orcid.pdf}}\hspace{1mm}Shichao Guo \\
	Department of Computer Science\\
	Aarhus University\\
	\texttt{au779770@uni.au.dk} \\
}

\renewcommand{\shorttitle}{Lightweight Neural Style Transfer}

\begin{document}
\maketitle

\begin{abstract}
This project investigates the efficacy of Depthwise Separable Convolutions (DSC) in reducing the computational cost of Neural Style Transfer. We implemented a baseline Fast Neural Style Transfer network and three progressive lightweight variants. Our experiments on the COCO 2017 dataset demonstrate that replacing standard convolutions with DSCs can reduce the model size by up to 87.62\% and achieve a 1.44x speedup on CPU inference, while maintaining comparable visual quality. This study highlights the potential of architectural optimizations for deploying generative models on edge devices.
\end{abstract}

\keywords{Neural Style Transfer \and Depthwise Separable Convolutions}

\section{Introduction}
Neural Style Transfer (NST), first introduced by Gatys et al. \citep{gatys2016image}, has become a popular application of deep learning, allowing users to blend the content of one image with the artistic style of another. While the original optimization-based approach was slow, Johnson et al. \citep{johnson2016perceptual} proposed a feed-forward network (Transformer Net) to generate stylized images in real-time. However, these networks often rely on heavy standard convolution layers, making them computationally expensive for real-time applications on mobile or edge devices.

The objective of this study is to determine if Depthwise Separable Convolutions---a technique popularized by MobileNet \citep{howard2017mobilenets} for efficient computing---can be applied to the style transfer domain to create a "lightweight" generator without significantly compromising artistic quality. We conduct a comparative study (Ablation Study) across three different levels of architectural modification to answer the question: Can we make style transfer lighter without losing the "style"?

\section{Related Work}
\textbf{Neural Style Transfer:} Gatys et al. \citep{gatys2016image} demonstrated that deep features from Convolutional Neural Networks (CNNs) can separate and recombine image content and style. Johnson et al. \citep{johnson2016perceptual} improved upon this by training a feed-forward network to approximate the optimization process, enabling real-time style transfer.

\textbf{Efficient Deep Learning:} To run deep models on mobile devices, reducing computational cost is crucial. Howard et al. \citep{howard2017mobilenets} introduced MobileNets, which utilize Depthwise Separable Convolutions (DSC) to factorize a standard convolution into a depthwise spatial convolution and a pointwise ($1\times1$) channel convolution. This factorization significantly reduces both parameters and computation (FLOPs).

Our work bridges these two fields by integrating DSCs into the architecture of Johnson et al., evaluating the trade-offs between efficiency and visual fidelity.

\section{Methods}

\subsection{Dataset and Preprocessing}
We utilized the \textbf{COCO 2017 Validation Set} as our content image source. Due to computational constraints, we used a subset of the data:
\begin{itemize}
    \item \textbf{Training Set:} 4,455 images
    \item \textbf{Validation Set:} 495 images (used for monitoring loss)
    \item \textbf{Test Set:} 50 images (reserved for final qualitative evaluation)
\end{itemize}
All images were resized to $256 \times 256$ pixels, center-cropped, and normalized. The style reference image used was Vincent van Gogh's \textit{The Starry Night} (See Figure \ref{fig:inputs}).

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{../history/starry_night.jpg}
        \caption{Style Image (Starry Night)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{../history/000000098497.jpg}
        \caption{Example Content Image}
    \end{subfigure}
    \caption{Inputs for the Style Transfer task.}
    \label{fig:inputs}
\end{figure}

\subsection{Model Architectures}
We designed four model variants to isolate the impact of the lightweight layers.

\textbf{Baseline:} A standard Transformer Net consisting of 3 convolution layers (Encoder), 5 Residual Blocks, and 3 Transposed Convolution layers (Decoder).

\textbf{Lightweight Layer Design:} We implemented a \textit{LightweightConvLayer} that replaces the standard spatial convolution with a Depthwise Convolution (groups=$C_{in}$) followed by a Pointwise Convolution ($1 \times 1$). Instance Normalization and ReLU activation were injected between the depthwise and pointwise stages to preserve non-linearity.

\textbf{The Variants:}
\begin{itemize}
    \item \textbf{Lightweight-v1:} Replaces only the 5 \textbf{Residual Blocks} with lightweight blocks.
    \item \textbf{Lightweight-v2:} Replaces the \textbf{Encoder} (initial downsampling layers) and \textbf{Residual Blocks}.
    \item \textbf{Lightweight-v3 (Fully Lightweight):} Replaces the \textbf{Encoder}, \textbf{Residual Blocks}, and the \textbf{Decoder} (upsampling layers).
\end{itemize}

\subsection{Training}
The networks were trained to minimize a Perceptual Loss function ($L_{total} = \lambda_c L_{content} + \lambda_s L_{style} + \lambda_{tv} L_{tv}$), computed using a pre-trained VGG-16 network.
\begin{itemize}
    \item \textbf{Optimization:} Adam optimizer ($lr=1e-3$) with \texttt{ReduceLROnPlateau} scheduler.
    \item \textbf{Training Duration:} 20 Epochs.
    \item \textbf{Hardware:} Training was performed on a GPU (A100), while inference benchmarking was conducted on a CPU to simulate edge constraints.
\end{itemize}

\section{Results}

\subsection{Quantitative Analysis: Efficiency}
We evaluated the models based on Parameter Count (Space complexity) and Inference Latency (Time complexity). As shown in Table \ref{tab:efficiency}, the \textbf{Lightweight-v3} model achieved a massive \textbf{87.6\% reduction in size}, shrinking the model from $\sim$1.68 million parameters to just $\sim$0.2 million. This translated to a consistent speedup, reducing inference time per image from $\sim$71ms to $\sim$49ms.

\begin{table}[h]
	\caption{Model Efficiency Comparison}
	\centering
	\begin{tabular}{ccccc}
		\toprule
		Model Variant     & Parameters & Reduction (\%) & Avg Latency (CPU) & Speedup  \\
		\midrule
		Baseline & 1,679,235 & - & 71.31 ms  & -   \\
		Lightweight-v1     & 381,315 & 77.29\% & 60.46 ms  & 1.18x    \\
		Lightweight-v2     & 292,796       & 82.56\%  & 57.65 ms & 1.24x \\
		Lightweight-v3     & 207,865      & 87.62\%  & 49.49 ms & 1.44x \\
		\bottomrule
	\end{tabular}
	\label{tab:efficiency}
\end{table}

\subsection{Training Dynamics}
We monitored the training process by tracking the total loss (content + style + variation) on both training and validation sets. As shown in Figure \ref{fig:loss}, all models demonstrated a consistent decrease in loss, indicating successful convergence without overfitting. The lightweight models (v1, v2, v3) showed a slightly slower convergence rate compared to the baseline, consistent with their reduced capacity.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../history/loss_train_total.png}
        \caption{Training Loss}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../history/loss_val_total.png}
        \caption{Validation Loss}
    \end{subfigure}
    \caption{Loss curves showing consistent convergence for all models.}
    \label{fig:loss}
\end{figure}

We also tracked the learning rate (Figure \ref{fig:lr}). The \texttt{ReduceLROnPlateau} scheduler reduced the learning rate when the validation loss plateaued, allowing for fine-grained optimization in later epochs.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{../history/train_learning_rate.png}
    \caption{Learning Rate Schedule}
    \label{fig:lr}
\end{figure}

\subsection{Qualitative Analysis: Visual Quality}
We compared the output images generated by the models on the test set.

\textbf{Early Training (Step 800):} As observed in Figure \ref{fig:step800}, the Baseline model learned the style features much faster than the lightweight variants. At step 800, the Baseline output is already recognizable, while v1, v2, and v3 are still struggling with basic color distribution. This confirms that the lightweight models have a "Capacity Gap" and converge slower.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{../history/800-baseline.png}
        \caption{Baseline (Step 800)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{../history/800-v1.png}
        \caption{Lightweight-v1 (Step 800)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{../history/800-v3.png}
        \caption{Lightweight-v3 (Step 800)}
    \end{subfigure}
    \caption{Early training comparison. The Baseline converges significantly faster.}
    \label{fig:step800}
\end{figure}

\textbf{Final Result:} Despite the slower start, all models converged successfully by the end of training. Figure \ref{fig:final_results} shows the final outputs. Visually, the lightweight models (v1, v2, v3) are almost indistinguishable from the baseline in terms of global structure and color. The residual bottleneck appears to be highly redundant, as removing it (v1) caused little to no visual degradation.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../history/000000098497_Baseline.jpg}
        \caption{Baseline Final Output}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../history/000000098497_v3.jpg}
        \caption{Lightweight-v3 Final Output}
    \end{subfigure}
    \caption{Final comparison on a test image. The fully lightweight model (v3) produces a highly similar artistic effect despite being 87\% smaller.}
    \label{fig:final_results}
\end{figure}

\subsection{Specific Artifacts}
A specific artifact was observed in the lightweight models: array-like red spots appeared within the yellow celestial orbs (see Figure \ref{fig:artifacts}), which were absent in the baseline. This reduces the subjective quality slightly in high-contrast regions.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{../history/000000098497_Baseline.jpg}
        \caption{Baseline}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{../history/000000098497_v1.jpg}
        \caption{v1 (Artifacts)}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{../history/000000098497_v2.jpg}
        \caption{v2 (Artifacts)}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{../history/000000098497_v3.jpg}
        \caption{v3 (Artifacts)}
    \end{subfigure}
    \caption{Detail view showing the "red spots" artifact in the yellow regions of the lightweight models.}
    \label{fig:artifacts}
\end{figure}

\section{Discussion}
\textbf{Capacity Gap and Convergence:} The lightweight models exhibited a slower convergence rate compared to the baseline. This is attributed to their reduced parameter capacity. However, the consistent decrease in both training and validation losses indicates that the lightweight architectures were successfully learning the style transfer task without suffering from overfitting.

\textbf{Artifacts from Decoupling:} We hypothesize that the "red spot" artifacts are a side effect of Depthwise Separable Convolutions. By decoupling spatial and channel-wise correlations, the network's ability to smooth out high-frequency color transitions locally might be reduced, leading to checkerboard-like patterns in specific style features.

\textbf{The Efficiency Trade-off:} While the lightweight variants introduced minor visual artifacts, they offered a compelling trade-off for deployment. The visual quality saturation towards the end of training suggests that 20 epochs were sufficient, and further training yielded diminishing returns.

\section{Conclusion}
We successfully demonstrated that replacing standard convolutions with depthwise separable convolutions is a highly effective strategy for Neural Style Transfer. Our \textbf{Lightweight-v3} model offers a practical solution for deployment, achieving an \textbf{87\% size reduction} and \textbf{1.44x faster inference} with minimal impact on visual fidelity. Future work could explore combining this approach with model quantization to further eliminate the observed artifacts and enhance performance.

\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}